Use GPU: cuda:0
0 enc_embedding.value_embedding.tokenConv.weight torch.Size([512, 7, 3])
0 enc_embedding.value_embedding.tokenConv.bias torch.Size([512])
enc_embedding.temporal_embedding.embed.weight torch.Size([512, 4])
enc_embedding.temporal_embedding.embed.bias torch.Size([512])
0 dec_embedding.value_embedding.tokenConv.weight torch.Size([512, 7, 3])
0 dec_embedding.value_embedding.tokenConv.bias torch.Size([512])
dec_embedding.temporal_embedding.embed.weight torch.Size([512, 4])
dec_embedding.temporal_embedding.embed.bias torch.Size([512])
encoder.attn_layers.0.attention.query_projection.weight torch.Size([512, 512])
encoder.attn_layers.0.attention.query_projection.bias torch.Size([512])
encoder.attn_layers.0.attention.key_projection.weight torch.Size([512, 512])
encoder.attn_layers.0.attention.key_projection.bias torch.Size([512])
encoder.attn_layers.0.attention.value_projection.weight torch.Size([512, 512])
encoder.attn_layers.0.attention.value_projection.bias torch.Size([512])
encoder.attn_layers.0.attention.out_projection.weight torch.Size([512, 512])
encoder.attn_layers.0.attention.out_projection.bias torch.Size([512])
0 encoder.attn_layers.0.conv1.weight torch.Size([2048, 512, 1])
0 encoder.attn_layers.0.conv1.bias torch.Size([2048])
0 encoder.attn_layers.0.conv2.weight torch.Size([512, 2048, 1])
0 encoder.attn_layers.0.conv2.bias torch.Size([512])
0 encoder.attn_layers.0.norm1.weight torch.Size([512])
0 encoder.attn_layers.0.norm1.bias torch.Size([512])
0 encoder.attn_layers.0.norm2.weight torch.Size([512])
0 encoder.attn_layers.0.norm2.bias torch.Size([512])
encoder.attn_layers.1.attention.query_projection.weight torch.Size([512, 512])
encoder.attn_layers.1.attention.query_projection.bias torch.Size([512])
encoder.attn_layers.1.attention.key_projection.weight torch.Size([512, 512])
encoder.attn_layers.1.attention.key_projection.bias torch.Size([512])
encoder.attn_layers.1.attention.value_projection.weight torch.Size([512, 512])
encoder.attn_layers.1.attention.value_projection.bias torch.Size([512])
encoder.attn_layers.1.attention.out_projection.weight torch.Size([512, 512])
encoder.attn_layers.1.attention.out_projection.bias torch.Size([512])
0 encoder.attn_layers.1.conv1.weight torch.Size([2048, 512, 1])
0 encoder.attn_layers.1.conv1.bias torch.Size([2048])
0 encoder.attn_layers.1.conv2.weight torch.Size([512, 2048, 1])
0 encoder.attn_layers.1.conv2.bias torch.Size([512])
0 encoder.attn_layers.1.norm1.weight torch.Size([512])
0 encoder.attn_layers.1.norm1.bias torch.Size([512])
0 encoder.attn_layers.1.norm2.weight torch.Size([512])
0 encoder.attn_layers.1.norm2.bias torch.Size([512])
0 encoder.conv_layers.0.downConv.weight torch.Size([512, 512, 3])
0 encoder.conv_layers.0.downConv.bias torch.Size([512])
0 encoder.conv_layers.0.norm.weight torch.Size([512])
0 encoder.conv_layers.0.norm.bias torch.Size([512])
0 encoder.norm.weight torch.Size([512])
0 encoder.norm.bias torch.Size([512])
decoder.layers.0.self_attention.query_projection.weight torch.Size([512, 512])
decoder.layers.0.self_attention.query_projection.bias torch.Size([512])
decoder.layers.0.self_attention.key_projection.weight torch.Size([512, 512])
decoder.layers.0.self_attention.key_projection.bias torch.Size([512])
decoder.layers.0.self_attention.value_projection.weight torch.Size([512, 512])
decoder.layers.0.self_attention.value_projection.bias torch.Size([512])
decoder.layers.0.self_attention.out_projection.weight torch.Size([512, 512])
decoder.layers.0.self_attention.out_projection.bias torch.Size([512])
decoder.layers.0.cross_attention.query_projection.weight torch.Size([512, 512])
decoder.layers.0.cross_attention.query_projection.bias torch.Size([512])
decoder.layers.0.cross_attention.key_projection.weight torch.Size([512, 512])
decoder.layers.0.cross_attention.key_projection.bias torch.Size([512])
decoder.layers.0.cross_attention.value_projection.weight torch.Size([512, 512])
decoder.layers.0.cross_attention.value_projection.bias torch.Size([512])
decoder.layers.0.cross_attention.out_projection.weight torch.Size([512, 512])
decoder.layers.0.cross_attention.out_projection.bias torch.Size([512])
0 decoder.layers.0.conv1.weight torch.Size([2048, 512, 1])
0 decoder.layers.0.conv1.bias torch.Size([2048])
0 decoder.layers.0.conv2.weight torch.Size([512, 2048, 1])
0 decoder.layers.0.conv2.bias torch.Size([512])
0 decoder.layers.0.norm1.weight torch.Size([512])
0 decoder.layers.0.norm1.bias torch.Size([512])
0 decoder.layers.0.norm2.weight torch.Size([512])
0 decoder.layers.0.norm2.bias torch.Size([512])
0 decoder.layers.0.norm3.weight torch.Size([512])
0 decoder.layers.0.norm3.bias torch.Size([512])
0 decoder.norm.weight torch.Size([512])
0 decoder.norm.bias torch.Size([512])
projection.weight torch.Size([7, 512])
projection.bias torch.Size([7])